{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import gc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import dask\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import s3fs\n",
    "import zarr\n",
    "import geocat.comp\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "from math import e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cooperative-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions\n",
    "#3D detrend function\n",
    "def detrend(x:np.ndarray,time:np.ndarray):\n",
    "        nt,nx,ny = x.shape\n",
    "        xtemp = x.reshape(nt,nx*ny)\n",
    "        p = np.polyfit(time, xtemp, deg=3)\n",
    "        fit = p[0]*(time[:,np.newaxis] **3)+ p[1]*(time[:,np.newaxis]**2) + p[2]*(time[:,np.newaxis]) + p[3]\n",
    "        return x - fit.reshape(nt,nx,ny)\n",
    "    \n",
    "#1D detrend function\n",
    "def altdetrend(x:np.ndarray,time:np.ndarray):\n",
    "        nt = x.shape\n",
    "        xtemp = x.reshape(nt)\n",
    "        p = np.polyfit(time, x, deg=1)\n",
    "        fit = p[0]*(time[:,np.newaxis])+ p[1]\n",
    "        return x - fit.reshape(nt)\n",
    "    \n",
    "def remove_time_mean(x):\n",
    "        return x - x.mean(dim='time')\n",
    "\n",
    "def removeSC(x):\n",
    "        return x.groupby('time.month').apply(remove_time_mean)\n",
    "\n",
    "# Calculate std normal anomaly\n",
    "def calStdNorAnom(x):\n",
    "    a=[]\n",
    "    for m in np.unique(x.time.dt.month):\n",
    "        mData=x[x.time.dt.month==m]\n",
    "        mRolling=mData.rolling(time=31, center=True).mean().bfill(dim=\"time\").ffill(dim=\"time\")\n",
    "        sRolling=mData.rolling(time=31, center=True).std().bfill(dim=\"time\").ffill(dim=\"time\")\n",
    "        normData=(mData-mRolling)/sRolling\n",
    "        a.append(normData)\n",
    "    combineArray=xr.concat(a,'time')\n",
    "    outArray=combineArray.sortby('time')\n",
    "    return outArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pharmaceutical-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS S3 storage\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "## By downloading the master CSV file enumerating all available data stores, we can interact with the spreadsheet\n",
    "## through a pandas DataFrame to search and explore for relevant data using the CMIP6 controlled vocabulary:\n",
    "df = pd.read_csv(\"https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "documented-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(qstring):\n",
    "    df_subset = df.query(qstring)\n",
    "    if df_subset.empty:\n",
    "        print('data not available for '+qstring)\n",
    "    else:\n",
    "        for v in df_subset.zstore.values:\n",
    "            zstore = v\n",
    "            mapper = fs.get_mapper(zstore)\n",
    "            return_ds = xr.open_zarr(mapper, consolidated=True)\n",
    "    return(return_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sapphire-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open CESM2 Ensemble Datasets\n",
    "modelName=\"'CESM2'\"\n",
    "institute=\"'NCAR'\"\n",
    "expList=[\"'historical'\"]\n",
    "actList=[\"'CMIP'\"]\n",
    "membList=[\"'r1i1p1f1'\", \"'r2i1p1f1'\", \"'r3i1p1f1'\", \"'r4i1p1f1'\" , \"'r5i1p1f1'\", \"'r6i1p1f1'\", \"'r7i1p1f1'\",\"'r8i1p1f1'\",\"'r9i1p1f1'\",\"'r10i1p1f1'\",\"'r11i1p1f1'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finished-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "files='Regrided_ERA5.nc'\n",
    "#Open ERA5 Dataset\n",
    "data=xr.open_dataset(files)\n",
    "cres=data.cres_Pre\n",
    "pr=data.pr\n",
    "land=data.lsMask\n",
    "\n",
    "\n",
    "#Select Monsoon Months\n",
    "months=[6,7,8,9]\n",
    "lagmonths=[6,7,8,9]\n",
    "#lagmonths=[2,3,4,5]\n",
    "#months=[8]\n",
    "#lagmonths=[5]\n",
    "#varOut.where(varOut.time.dt.month.isin(months), drop=True) #Change varOut to desired variable\n",
    "prec=pr.where(pr.time.dt.month.isin(months), drop=True)\n",
    "land=land.where(land.time.dt.month.isin(months), drop=True)\n",
    "cres=cres.where(cres.time.dt.month.isin(lagmonths), drop=True)\n",
    "\n",
    "\n",
    "#Select only the SAM lat,lon range: 60-100E, 10-30N\n",
    "precip=prec.sel(lon=slice(60,100),lat=slice(10,30))\n",
    "land=land.sel(lon=slice(60,100),lat=slice(10,30))\n",
    "precip=xr.where(land==0,np.nan,precip) #remove oceans, monsoon is defined as only over land \n",
    "\n",
    "#Do weighted correction on precipitation\n",
    "weights=np.cos(np.deg2rad(precip.lat))\n",
    "prec_index=precip.weighted(weights).mean(dim=('lat','lon'))\n",
    "prec_index=prec_index*60*60*24 #conversion to mm/day, exluding dividing by rho and multiplying by 1000mm/m\n",
    "\n",
    "#Remove seasonal cycle\n",
    "prec_index=removeSC(prec_index)\n",
    "\n",
    "#Normalize\n",
    "prec_index=calStdNorAnom(prec_index)\n",
    "\n",
    "#Detrend data sets\n",
    "time=prec_index.time\n",
    "prec_index=prec_index.to_numpy()\n",
    "time=time.to_numpy()\n",
    "time=time.astype(int)/10**9\n",
    "\n",
    "prec_index=altdetrend(prec_index,time)\n",
    "prec_index=xr.DataArray(prec_index,coords=[time],dims=['time'])\n",
    "\n",
    "\n",
    "pr_new,cres = xr.broadcast(prec_index,cres) #broadcast pr_mean array to fill array to allow regression to be executed\n",
    "\n",
    "pr_new=pr_new.dropna(dim='time')\n",
    "cres=cres.dropna(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "general-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca=CCA(n_components=1)\n",
    "testcca=[]\n",
    "for i in range(len(cres.time)):\n",
    "    for j in range (len(pr_new.time)):\n",
    "        U_c , V_c =cca.fit_transform(cres[i],pr_new[j])\n",
    "        \n",
    "        result=np.corrcoef(U_c.T,V_c.T)[0,1]\n",
    "        \n",
    "        #print(result)\n",
    "        \n",
    "        testcca.append(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "offshore-battle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 181, 240)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "computational-absorption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29584"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testcca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "cres=cres.to_numpy()\n",
    "ntime, nlat, nlon = cres.shape\n",
    "\n",
    "d2_cres=cres.reshape((ntime,nlat*nlon))\n",
    "\n",
    "d2_cres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "cca=CCA(n_components=1)\n",
    "\n",
    "x_c, y_c =cca.fit_transform(d2_cres,yIn)\n",
    "\n",
    "\n",
    "result = np.corrcoef(x_c.T, y_c.T)#[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-lighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-distributor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-result",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "creslist=[]\n",
    "crellist=[]\n",
    "netTOAcslist=[]\n",
    "dummy_ylist=[]\n",
    "\n",
    "for j,memb in enumerate(membList):\n",
    "        inputStr  = \"institution_id =='NCAR' & source_id=='CESM2' & table_id=='Amon' & experiment_id=='historical' &  member_id==\"+memb+\" & variable_id==\"\n",
    "        altinputStr  = \"activity_id=='CMIP' & table_id=='fx' & source_id=='CESM2' & experiment_id=='historical' &  member_id==\"+memb+\" & variable_id==\"\n",
    "        rsut_ds   = getData(inputStr+\"'rsut'\")\n",
    "        pr_ds     = getData(inputStr+\"'pr'\")\n",
    "        rsutcs_ds = getData(inputStr+\"'rsutcs'\")\n",
    "        sftlf_ds = getData(altinputStr+\"'sftlf'\")\n",
    "        rlut_ds   =getData(inputStr+\"'rlut'\")\n",
    "        rlutcs_ds  =getData(inputStr+\"'rlutcs'\")\n",
    "        rsdt_ds  =getData(inputStr+\"'rsdt'\")\n",
    "        \n",
    "        netTOAcs = rsdt_ds.rsdt - rsutcs_ds.rsutcs - rlutcs_ds.rlutcs\n",
    "        \n",
    "        cres= rsutcs_ds.rsutcs-rsut_ds.rsut\n",
    "        crel=rlutcs_ds.rlutcs-rlut_ds.rlut\n",
    "        prec=pr_ds.pr\n",
    "        land=sftlf_ds.sftlf\n",
    "    \n",
    "        datetimeindex=cres.indexes['time'].to_datetimeindex()\n",
    "        cres['time']=datetimeindex\n",
    "        crel['time']=datetimeindex\n",
    "        netTOAcs['time']=datetimeindex\n",
    "        prec['time']=datetimeindex\n",
    "        \n",
    "        cres,land= xr.broadcast(cres,land) #add time dimension to land variable for compatability with CRE variables\n",
    "        \n",
    "        \n",
    "        \n",
    "        months=[6,7,8,9]\n",
    "        leadmonths=[6,7,8,9]\n",
    "\n",
    "        #varOut.where(varOut.time.dt.month.isin(months), drop=True) #Change varOut to desired variable\n",
    "        prec=prec.sel(time=prec.time.dt.month.isin(months))\n",
    "        cres=cres.sel(time=cres.time.dt.month.isin(leadmonths))\n",
    "        crel=crel.sel(time=crel.time.dt.month.isin(leadmonths))\n",
    "        netTOAcs=netTOAcs.sel(time=netTOAcs.time.dt.month.isin(leadmonths))\n",
    "        land=land.sel(time=land.time.dt.month.isin(months))\n",
    "        \n",
    "        #Select only the SAM lat,lon range: 60-100E, 10-30N\n",
    "        precip=prec.sel(lon=slice(60,100),lat=slice(10,30))\n",
    "        land=land.sel(lon=slice(60,100),lat=slice(10,30))\n",
    "        \n",
    "        precip=xr.where(land==0,np.nan,precip) #remove oceans, monsoon is defined as only over land \n",
    "        \n",
    "        #Do weighted correction on precipitation\n",
    "        weights=np.cos(np.deg2rad(precip.lat))\n",
    "        prec_index=precip.weighted(weights).mean(dim=('lat','lon'))\n",
    "        prec_index=prec_index*60*60*24 #conversion to mm/day, exluding dividing by rho and multiplying by 1000mm/m\n",
    "        \n",
    "        lat=cres.lat\n",
    "        lon=cres.lon\n",
    "        \n",
    "        #Do the preprocessing\n",
    "        cres=removeSC(cres)\n",
    "        cres=calStdNorAnom(cres)\n",
    "        \n",
    "        crel=removeSC(crel)\n",
    "        crel=calStdNorAnom(crel)\n",
    "        \n",
    "        netTOAcs=removeSC(netTOAcs)\n",
    "        netTOAcs=calStdNorAnom(netTOAcs)\n",
    "        \n",
    "        #Detrend\n",
    "        time=cres.time\n",
    "        cres=cres.to_numpy()\n",
    "        crel=crel.to_numpy()\n",
    "        netTOAcs=netTOAcs.to_numpy()\n",
    "        time=time.to_numpy()\n",
    "        time=time.astype(int)/10**9\n",
    "        \n",
    "        cres=detrend(cres,time)\n",
    "        cres=xr.DataArray(cres,coords=[time,lat,lon],dims=['time','lat','lon'])\n",
    "        \n",
    "        crel=detrend(crel,time)\n",
    "        crel=xr.DataArray(crel,coords=[time,lat,lon],dims=['time','lat','lon'])\n",
    "        \n",
    "        netTOAcs=detrend(netTOAcs,time)\n",
    "        netTOAcs=xr.DataArray(netTOAcs,coords=[time,lat,lon],dims=['time','lat','lon'])\n",
    "        \n",
    "        #Precip preprocessing\n",
    "        #Remove seasonal cycle\n",
    "        prec_index=removeSC(prec_index)\n",
    "        \n",
    "        #Normalize\n",
    "        prec_index=calStdNorAnom(prec_index)\n",
    "        \n",
    "        #Detrend\n",
    "        time=prec_index.time\n",
    "        prec_index=prec_index.to_numpy()\n",
    "        time=time.to_numpy()\n",
    "        time=time.astype(int)/10**9\n",
    "        \n",
    "        prec_index=altdetrend(prec_index,time)\n",
    "        prec_index=xr.DataArray(prec_index,coords=[time],dims=['time'])\n",
    "        \n",
    "        mysd=prec_index.std()\n",
    "        mymean=prec_index.mean()\n",
    "\n",
    "        buckets=pd.Categorical(pd.cut(prec_index, [mymean - mysd* 10000,  mymean - 0.5*mysd,  mymean + 0.5*mysd, mymean + mysd* 10000])).rename_categories(['low','average','high'])\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(buckets)\n",
    "\n",
    "        labelprec=le.transform(buckets)\n",
    "\n",
    "        # convert integers to dummy variables (i.e. one hot encoded)\n",
    "        nclasses=3\n",
    "        dummy_y=to_categorical(labelprec,nclasses) #converts to binary\n",
    "        \n",
    "        creslist.append(cres)\n",
    "        cresstack=np.stack(creslist,axis=0)\n",
    "        \n",
    "        crellist.append(crel)\n",
    "        crelstack=np.stack(crellist,axis=0)\n",
    "        \n",
    "        netTOAcslist.append(netTOAcs)\n",
    "        netTOAcsstack=np.stack(netTOAcslist,axis=0)\n",
    "        \n",
    "        dummy_ylist.append(dummy_y)\n",
    "        dummy_ystack=np.stack(dummy_ylist,axis=0)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate each cres array\n",
    "cresIn1=cresstack[0,:,:,:]\n",
    "cresIn2=cresstack[1,:,:,:]\n",
    "cresIn3=cresstack[2,:,:,:]\n",
    "cresIn4=cresstack[3,:,:,:]\n",
    "cresIn5=cresstack[4,:,:,:]\n",
    "cresIn6=cresstack[5,:,:,:]\n",
    "cresIn7=cresstack[6,:,:,:]\n",
    "cresIn8=cresstack[7,:,:,:]\n",
    "cresIn9=cresstack[8,:,:,:]\n",
    "cresIn10=cresstack[9,:,:,:]\n",
    "cresIn11=cresstack[10,:,:,:]\n",
    "\n",
    "\n",
    "#Concatenate the ensemble members along time axis\n",
    "cresIn=np.concatenate((cresIn1,cresIn2,cresIn3,cresIn4,cresIn5,cresIn6,cresIn7,cresIn8,cresIn9,cresIn10,cresIn11),axis=0)\n",
    "\n",
    "#Seperate each crel array\n",
    "crelIn1=crelstack[0,:,:,:]\n",
    "crelIn2=crelstack[1,:,:,:]\n",
    "crelIn3=crelstack[2,:,:,:]\n",
    "crelIn4=crelstack[3,:,:,:]\n",
    "crelIn5=crelstack[4,:,:,:]\n",
    "crelIn6=crelstack[5,:,:,:]\n",
    "crelIn7=crelstack[6,:,:,:]\n",
    "crelIn8=crelstack[7,:,:,:]\n",
    "crelIn9=crelstack[8,:,:,:]\n",
    "crelIn10=crelstack[9,:,:,:]\n",
    "crelIn11=crelstack[10,:,:,:]\n",
    "\n",
    "#Concatenate the ensemble members along time axis\n",
    "crelIn=np.concatenate((crelIn1,crelIn2,crelIn3,crelIn4,crelIn5,crelIn6,crelIn7,crelIn8,crelIn9,crelIn10,crelIn11),axis=0)\n",
    "\n",
    "#Seperate each netTOAcs array\n",
    "netTOAcsIn1=netTOAcsstack[0,:,:,:]\n",
    "netTOAcsIn2=netTOAcsstack[1,:,:,:]\n",
    "netTOAcsIn3=netTOAcsstack[2,:,:,:]\n",
    "netTOAcsIn4=netTOAcsstack[3,:,:,:]\n",
    "netTOAcsIn5=netTOAcsstack[4,:,:,:]\n",
    "netTOAcsIn6=netTOAcsstack[5,:,:,:]\n",
    "netTOAcsIn7=netTOAcsstack[6,:,:,:]\n",
    "netTOAcsIn8=netTOAcsstack[7,:,:,:]\n",
    "netTOAcsIn9=netTOAcsstack[8,:,:,:]\n",
    "netTOAcsIn10=netTOAcsstack[9,:,:,:]\n",
    "netTOAcsIn11=netTOAcsstack[10,:,:,:]\n",
    "\n",
    "#Concatenate the ensemble members along time axis\n",
    "netTOAcsIn=np.concatenate((netTOAcsIn1,netTOAcsIn2,netTOAcsIn3,netTOAcsIn4,netTOAcsIn5,netTOAcsIn6,netTOAcsIn7,netTOAcsIn8,netTOAcsIn9,netTOAcsIn10,netTOAcsIn11),axis=0)\n",
    "\n",
    "#Seperate each precip_index array\n",
    "yIn1=dummy_ystack[0,:,:]\n",
    "yIn2=dummy_ystack[1,:,:]\n",
    "yIn3=dummy_ystack[2,:,:]\n",
    "yIn4=dummy_ystack[3,:,:]\n",
    "yIn5=dummy_ystack[4,:,:]\n",
    "yIn6=dummy_ystack[5,:,:]\n",
    "yIn7=dummy_ystack[6,:,:]\n",
    "yIn8=dummy_ystack[7,:,:]\n",
    "yIn9=dummy_ystack[8,:,:]\n",
    "yIn10=dummy_ystack[9,:,:]\n",
    "yIn11=dummy_ystack[10,:,:]\n",
    "\n",
    "#Concatenate the ensemble members along time axis\n",
    "yIn=np.concatenate((yIn1,yIn2,yIn3,yIn4,yIn5,yIn6,yIn7,yIn8,yIn9,yIn10,yIn11),axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_gpu",
   "language": "python",
   "name": "test_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
